{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This Kernel is an iplementation of CNN Architectures using VGG16 and VGG19 CNN architectures**\n",
    "\n",
    "The Kernel Architeture is described as:\n",
    "<img src=\"https://qphs.fs.quoracdn.net/main-qimg-83c7dee9e8b039c3ca27c8dd91cacbb4\" width=\"900px\">\n",
    "\n",
    "\n",
    "**Important Features of VGG16 architectures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fruits', 'dogs-vs-cats-redux-kernels-edition', 'flowers-recognition', 'vgg16']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have used Python's Keras library to implement the model. The following code shows the implementation of the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are two ways to build Keras models: sequential and functional. **\n",
    "\n",
    "The sequential API allows you to create models layer-by-layer for most problems. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs.\n",
    "\n",
    "Alternatively, the functional API allows you to create models that have a lot more flexibility as you can easily define models where layers connect to more than just the previous and next layers. In fact, you can connect layers to (literally) any other layer. As a result, creating complex networks such as siamese networks and residual networks become possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from keras.layers import Input, Conv2D, MaxPool2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "# Define the input\n",
    "#   Unlike the Sequential model, you must create and define \n",
    "#   a standalone \"Input\" layer that specifies the shape of input \n",
    "#   data. The input layer takes a \"shape\" argument, which is a \n",
    "#   tuple that indicates the dimensionality of the input data.\n",
    "#   When input data is one-dimensional, such as the MLP, the shape \n",
    "#   must explicitly leave room for the shape of the mini-batch size \n",
    "#   used when splitting the data when training the network. Hence, \n",
    "#   the shape tuple is always defined with a hanging last dimension.\n",
    "\n",
    "_input = Input((224,224,1))\n",
    "\n",
    "conv1 = Conv2D(filters=64, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(_input)\n",
    "conv2 = Conv2D(filters=64, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(conv1)\n",
    "pool1 = MaxPool2D((2,2))(conv2)\n",
    "\n",
    "\n",
    "conv3 = Conv2D(filters=128, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(pool1)\n",
    "conv4 = Conv2D(filters=128, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(conv3)\n",
    "pool2 = MaxPool2D((2,2))(conv4)\n",
    "\n",
    "\n",
    "conv5 = Conv2D(filters=256, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(pool2)\n",
    "conv6 = Conv2D(filters=256, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(conv5)\n",
    "conv7 = Conv2D(filters=256, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(conv6)\n",
    "pool3 = MaxPool2D((2,2))(conv7)\n",
    "\n",
    "\n",
    "conv8 = Conv2D(filters=512, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(pool3)\n",
    "conv9 = Conv2D(filters=512, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(conv8)\n",
    "conv10 = Conv2D(filters=512, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(conv9)\n",
    "pool4 = MaxPool2D((2,2))(conv10)\n",
    "\n",
    "\n",
    "conv11 = Conv2D(filters=512, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(pool4)\n",
    "conv12 = Conv2D(filters=512, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(conv11)\n",
    "conv13 = Conv2D(filters=512, kernel_size=(3,3), padding = \"same\", activation=\"relu\")(conv12)\n",
    "pool5 = MaxPool2D((2,2))(conv13)\n",
    "\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "\n",
    "dense1 = Dense(4096, activation=\"relu\")(flat)\n",
    "dense2 = Dense(4096, activation=\"relu\")(dense1)\n",
    "output = Dense(1000, activation=\"softmax\")(dense2)\n",
    "\n",
    "vgg16_model = Model(inputs = _input, outputs= output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pretrained Model of VGG16**\n",
    "\n",
    "Keras Library provides VGG16 pretrained model so that one can save time and use them for different usages:\n",
    "1. Transfer Learning\n",
    "2. Features extraction from Images\n",
    "3. Object Detection\n",
    "\n",
    "We can add weights to respective layers after loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import decode_predictions, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# loadingImages\n",
    "catImage1 = \"../input/dogs-vs-cats-redux-kernels-edition/train/cat.10032.jpg\"\n",
    "dogImage1 = \"../input/dogs-vs-cats-redux-kernels-edition/train/dog.10042.jpg\"\n",
    "flowerImage1 = \"../input/flowers-recognition/flowers/flowers/rose/16078501836_3ac067e18a.jpg\"\n",
    "fruit1 = \"../input/fruits/fruits-360_dataset/fruits-360/Training/Hazelnut/308_100.jpg\"\n",
    "\n",
    "images = [catImage1, dogImage1, flowerImage1, fruit1]\n",
    "\n",
    "def loadImage(path):\n",
    "    # changing dimensions to (224,224)     \n",
    "    img = image.load_img(path, target_size=(224,224))\n",
    "    # converting image to array\n",
    "    img = image.img_to_array(img)\n",
    "    # Expand the shape of an array.\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
